{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2ee8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Desktop\\10 Acadamy\\week2\\Customer-Experience-Analytics-for-Fintech-Apps\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "#sentiment analysis\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#keyword Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ec153f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ca59dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Administrator\\Desktop\\10 Acadamy\\week2\\Customer-Experience-Analytics-for-Fintech-Apps\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION & SETUP\n",
    "# ---------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentiment_model = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "DATA_PATH = \"../data/clean_reviews.csv\"  # Expected columns: ['review_id', 'bank', 'rating', 'review_text']\n",
    "OUTPUT_PATH = \"../data/task2_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b8accb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# TEXT PREPROCESSING FUNCTION\n",
    "# ---------------------------\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text.lower())\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5971d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# SENTIMENT ANALYSIS\n",
    "# ---------------------------\n",
    "def compute_sentiment(texts):\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        try:\n",
    "            result = sentiment_model(text[:512])[0]  # Truncate long reviews\n",
    "            score = result['score'] if result['label'] == 'POSITIVE' else -result['score']\n",
    "            label = result['label'].lower()\n",
    "        except Exception:\n",
    "            score, label = 0, 'neutral'\n",
    "        results.append((label, score))\n",
    "    return zip(*results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a0b803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# TF-IDF KEYWORD EXTRACTION\n",
    "# ---------------------------\n",
    "def extract_keywords(texts, top_n=10):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    keywords = []\n",
    "    for i in range(tfidf_matrix.shape[0]):\n",
    "        row = tfidf_matrix.getrow(i).toarray().flatten()\n",
    "        top_indices = row.argsort()[-top_n:][::-1]\n",
    "        top_features = [feature_names[idx] for idx in top_indices if row[idx] > 0]\n",
    "        keywords.append(top_features)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21c5a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# THEME CLUSTERING (RULE-BASED)\n",
    "# ---------------------------\n",
    "def assign_themes(keywords):\n",
    "    theme_dict = {\n",
    "        \"Account Access Issues\": [\"login\", \"password\", \"authentication\", \"signin\", \"access\"],\n",
    "        \"Transaction Performance\": [\"transfer\", \"delay\", \"transaction\", \"payment\", \"fail\"],\n",
    "        \"User Interface & Experience\": [\"interface\", \"design\", \"ui\", \"navigation\", \"friendly\"],\n",
    "        \"Customer Support\": [\"support\", \"help\", \"service\", \"response\", \"agent\"],\n",
    "        \"Feature Requests\": [\"feature\", \"add\", \"request\", \"need\", \"option\"]\n",
    "    }\n",
    "\n",
    "    def get_themes(kws):\n",
    "        assigned = set()\n",
    "        for kw in kws:\n",
    "            for theme, triggers in theme_dict.items():\n",
    "                if any(trigger in kw for trigger in triggers):\n",
    "                    assigned.add(theme)\n",
    "        return list(assigned) if assigned else [\"Other\"]\n",
    "\n",
    "    return [get_themes(kws) for kws in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b968a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# MAIN ANALYSIS PIPELINE\n",
    "# ---------------------------\n",
    "def main():\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    df = df.dropna(subset=[\"review\"])\n",
    "\n",
    "    # Preprocessing\n",
    "    df[\"clean_text\"] = df[\"review\"].apply(preprocess_text)\n",
    "\n",
    "    # Sentiment\n",
    "    sentiments = df[\"review\"].apply(compute_sentiment)\n",
    "    df[\"sentiment_label\"] = [label for label, _ in sentiments]\n",
    "    df[\"sentiment_score\"] = [score for _, score in sentiments]\n",
    "\n",
    "    # Keywords and Themes\n",
    "    df[\"keywords\"] = extract_keywords(df[\"clean_text\"])\n",
    "    df[\"themes\"] = df[\"keywords\"].apply(assign_themes)\n",
    "\n",
    "    # Save\n",
    "    df_out = df[[\"review\", \"bank\", \"rating\", \"sentiment_label\", \"sentiment_score\", \"themes\"]]\n",
    "    df_out.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"âœ… Task 2 complete. Output saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d416245c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
